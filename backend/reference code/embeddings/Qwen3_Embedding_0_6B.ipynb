{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXPy4MYI3yra"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install sentence_transformers\n",
        "# Requires transformers>=4.51.0\n",
        "# Requires sentence-transformers>=2.7.0\n",
        "# %pip install vllm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1rqe9db73yrc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.03928966  0.01477094 -0.00496417 ... -0.00500184  0.01867614\n",
            "   0.00548171]\n",
            " [-0.00426361  0.01462148 -0.00627571 ... -0.02012267 -0.02811135\n",
            "  -0.00256997]\n",
            " [ 0.03132604 -0.03730356 -0.00269909 ...  0.01760445  0.00872107\n",
            "   0.00805308]]\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "sentences = [\n",
        "    \"The weather is lovely today.\",\n",
        "    \"It's so sunny outside!\",\n",
        "    \"He drove to the stadium.\"\n",
        "]\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)\n",
        "\n",
        "similarities = model.similarity(embeddings, embeddings)\n",
        "print(similarities.shape)\n",
        "# [3, 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7646, 0.1414],\n",
            "        [0.1355, 0.6000]])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n",
        "# together with setting `padding_side` to \"left\":\n",
        "# model = SentenceTransformer(\n",
        "#     \"Qwen/Qwen3-Embedding-0.6B\",\n",
        "#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n",
        "#     tokenizer_kwargs={\"padding_side\": \"left\"},\n",
        "# )\n",
        "\n",
        "# The queries and documents to embed\n",
        "queries = [\n",
        "    \"What is the capital of China?\",\n",
        "    \"Explain gravity\",\n",
        "]\n",
        "documents = [\n",
        "    \"The capital of China is Beijing.\",\n",
        "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
        "]\n",
        "\n",
        "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
        "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
        "# also pass your own prompt via the `prompt` argument\n",
        "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
        "document_embeddings = model.encode(documents)\n",
        "\n",
        "# Compute the (cosine) similarity between the query and document embeddings\n",
        "similarity = model.similarity(query_embeddings, document_embeddings)\n",
        "print(similarity)\n",
        "# tensor([[0.7646, 0.1414],\n",
        "#         [0.1355, 0.6000]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGNHRTL63yrd"
      },
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
        "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52MP6Kcd3yre"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'your_token'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1muser: \u001b[0m MukeshDevrath007\n"
          ]
        }
      ],
      "source": [
        "!hf auth whoami"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lthxArtr3yre"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "model = AutoModel.from_pretrained(\"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    \"Today is a sunny day and I will get some ice cream.\",\n",
        "    return_tensors=\"pt\", padding=True, truncation=True, max_length=8192\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "print(embeddings.shape)  # torch.Size([1, 1024])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
